{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from keras.applications.vgg16 import preprocess_input as preprocess_input_16\n",
    "from keras.applications.vgg19 import preprocess_input as preprocess_input_19\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "import pickle\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../places_keras/Keras-VGG16-places365/\"))\n",
    "from vgg16_places_365 import VGG16_Places365\n",
    "from vgg16_hybrid_places_1365 import VGG16_Hubrid_1365\n",
    "\n",
    "# Constants, paths, etc\n",
    "NUM_AUGMENTATION = 14\n",
    "TRAIN_PATH = '../input/train/'\n",
    "VAL_PATH = '../input/val/'\n",
    "TEST_PATH = '../input/test/'\n",
    "AUG_TRAIN_PATH = '../input/aug/train/'\n",
    "AUG_VAL_PATH = '../input/aug/val/'\n",
    "AUG_TEST_PATH = '../input/aug/test/'\n",
    "PATH_FEATURES = '../features/'\n",
    "PICKLE_PATH = '../pickle/'\n",
    "TEST_ANSWERS = '../input/test_answers.csv'\n",
    "\n",
    "\n",
    "# Separate 20% of the files for validation\n",
    "def separate_for_validation(city):\n",
    "    l = []\n",
    "    for filename in tqdm(os.listdir('../input/train/{}'.format(city))):\n",
    "        l.append(filename)\n",
    "\n",
    "    for f in np.random.choice(l, size=int(0.2*len(l)), replace=False):\n",
    "        print('mv {} ../../val/{}/'.format(f, city))\n",
    "\n",
    "# Read pre-computed descriptors and return them\n",
    "def read_vgg_features(dataset=\"train\", input_name=''):\n",
    "    x = np.loadtxt('{}{}_{}_{}.txt'.format(PATH_FEATURES, \"x\", dataset, input_name))\n",
    "    y = np.loadtxt('{}{}_{}_{}.txt'.format(PATH_FEATURES, \"y\", dataset, input_name))\n",
    "    return x, y\n",
    "\n",
    "# Compute normalized accuracy\n",
    "def normalized_accuracy(y_true, y_pred):\n",
    "    conf = confusion_matrix(y_true, y_pred)\n",
    "    norm_acc = 0.0\n",
    "    for i in range(conf.shape[0]):\n",
    "        norm_acc += (conf[i][i] / np.sum(conf[i,:]))\n",
    "    return norm_acc / (conf.shape[0])\n",
    "\n",
    "# Pass an image through the network and return its feature vector\n",
    "def get_vgg_feature_for_image(model, img_path, weights):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    if weights == 'places' or weights == 'places_2':\n",
    "        x = preprocess_input_16(x)        \n",
    "    elif weights == 'imagenet':\n",
    "        x = preprocess_input_19(x)\n",
    "    return model.predict(x)\n",
    "\n",
    "# Perform data augmentation in a single image\n",
    "def augment(image, filename, path, save_output=False):\n",
    "    aug_images = []\n",
    "    rows, cols,_ = image.shape\n",
    "\n",
    "    # Original image\n",
    "    aug_images.append(image)\n",
    "    \n",
    "    # Flip\n",
    "    aug_images.append(cv2.flip(image, 1))\n",
    "    \n",
    "    # Rotate\n",
    "    angles = [-20, -10, 10, 20]\n",
    "    for angle in angles:\n",
    "        M = cv2.getRotationMatrix2D((cols/2,rows/2), angle, 1)\n",
    "        aug_images.append(cv2.warpAffine(image,M,(cols,rows)))\n",
    "\n",
    "    # Brightness\n",
    "    brightness_values = [-25, -12, 12, 25]  \n",
    "    for brightness in brightness_values:\n",
    "        changed = np.add(image.astype('uint32'), brightness)\n",
    "        changed = changed.clip(0, 255).astype('uint8')\n",
    "        aug_images.append(changed)    \n",
    "    \n",
    "    # Contrast\n",
    "    contrast_values = [0.8, 0.9, 1.1, 1.2]    \n",
    "    for contrast in contrast_values:\n",
    "        changed = image.astype('uint32') * contrast\n",
    "        changed = changed.clip(0, 255).astype('uint8')\n",
    "        aug_images.append(changed)\n",
    "    \n",
    "    if save_output:\n",
    "        for i, aug_image in enumerate(aug_images):\n",
    "            cv2.imwrite(\"{}{}_{:02d}.jpg\".format(path, filename.replace('.jpg', ''), i), aug_image)\n",
    "    return aug_images\n",
    "\n",
    "# Perform augmentation in the given dataset\n",
    "def augment_and_save(dataset=\"train\"):\n",
    "    if dataset == \"train\":\n",
    "        input_path = TRAIN_PATH\n",
    "        output_path = AUG_TRAIN_PATH        \n",
    "    elif dataset == \"val\":\n",
    "        input_path = VAL_PATH\n",
    "        output_path = AUG_VAL_PATH        \n",
    "    elif dataset == \"test\":\n",
    "        input_path = TEST_PATH\n",
    "        output_path = AUG_TEST_PATH        \n",
    "    else:\n",
    "        raise ValueError(\"dataset must be train, val or test\")\n",
    "\n",
    "    for filename in tqdm(os.listdir(input_path)):\n",
    "        # Avoid temporary MAC files, etc\n",
    "        if filename.endswith('.jpg'):\n",
    "            image = cv2.imread(input_path + filename)\n",
    "            augment(image, filename, output_path, save_output=True)\n",
    "            \n",
    "# Get desired path for the augmented images\n",
    "def get_augmented_path(filename, aug_path):\n",
    "    path = []\n",
    "    for i in range(NUM_AUGMENTATION):\n",
    "        path.append(\"{}{}_{:02d}.jpg\".format(aug_path, filename.replace('.jpg', ''), i))\n",
    "    return path\n",
    "\n",
    "# Read test answers from CSV file and return them as a dictionary\n",
    "def get_test_answers():\n",
    "    file = open(TEST_ANSWERS, 'r') \n",
    "    answers = dict()\n",
    "    next(file)\n",
    "    \n",
    "    str_to_num = dict()\n",
    "    str_to_num[\"boston_marathon\"] = 0\n",
    "    str_to_num[\"austin_marathon\"] = 1\n",
    "    str_to_num[\"occupy_baltimore\"] = 2\n",
    "    str_to_num[\"occupy_portland\"] = 3\n",
    "\n",
    "    for line in file:\n",
    "        val = line.split('|')\n",
    "        name = val[1]\n",
    "        true = val[-1].replace('\\n', '')\n",
    "\n",
    "        if true == \"Y\":\n",
    "            answer = val[-2].replace('\\n', '')\n",
    "            answers[name] = str_to_num[answer]     \n",
    "    return answers\n",
    "\n",
    "\n",
    "# Describe images in train/test/val dataset using the method provided. Write results to 'features' folder\n",
    "def get_vgg_features(dataset, output_name='', use_augmentation=False, weights='places'):\n",
    "    if dataset == \"train\":\n",
    "        path = TRAIN_PATH\n",
    "        aug_path = AUG_TRAIN_PATH\n",
    "    elif dataset == \"val\":\n",
    "        path = VAL_PATH\n",
    "        aug_path = AUG_VAL_PATH\n",
    "    elif dataset == \"test\":\n",
    "        path = TEST_PATH\n",
    "        aug_path = AUG_TEST_PATH\n",
    "        answers = get_test_answers()\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be train, val or test\")\n",
    "\n",
    "    filenames = []\n",
    "    x, y = np.empty(0), np.empty(0)\n",
    "\n",
    "    if weights == 'places':\n",
    "        base_model = VGG16_Hubrid_1365(weights='places')\n",
    "    elif weights == 'imagenet':\n",
    "        base_model = VGG19(weights='imagenet')\n",
    "    elif weights == 'places_2':\n",
    "        base_model = VGG16_Places365(weights='places')\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n",
    "\n",
    "    i = 0\n",
    "    for filename in tqdm(os.listdir(path)):\n",
    "        # Avoid temporary MAC files, etc\n",
    "        if filename.endswith('.jpg'):\n",
    "            image = cv2.imread(path + filename)\n",
    "            input_images = []\n",
    "            \n",
    "            filenames.append(filename)\n",
    "            if use_augmentation:\n",
    "                input_images = get_augmented_path(filename, aug_path)\n",
    "            else:\n",
    "                input_images.append(path + filename)\n",
    "            \n",
    "            for image in input_images:\n",
    "                if dataset == \"train\" or dataset == \"val\":\n",
    "                    y = np.append(y, int(filename[:2]))\n",
    "                else:\n",
    "                    y = np.append(y, answers[filename.replace('.jpg', '')])\n",
    "\n",
    "                descriptor = get_vgg_feature_for_image(model, image, weights)\n",
    "                if x.size == 0:\n",
    "                    x = descriptor.copy()\n",
    "                else:\n",
    "                    x = np.vstack((x, descriptor))\n",
    "\n",
    "    np.savetxt('{}{}_{}_{}.txt'.format(PATH_FEATURES, \"x\", dataset, output_name), x)\n",
    "    np.savetxt('{}{}_{}_{}.txt'.format(PATH_FEATURES, \"y\", dataset, output_name), y)\n",
    "    return x, y, filenames\n",
    "\n",
    "\n",
    "# UNCOMMENT TO EXTRACT FEATURES FROM THE NETWORK\n",
    "#\n",
    "# x_train, y_train, _ = get_vgg_features(dataset=\"train\", output_name=\"places_4class\")\n",
    "# x_val, y_val, _ = get_vgg_features(dataset=\"val\", output_name=\"places_4class\")\n",
    "\n",
    "# x_train_img, y_train_img, _ = get_vgg_features(dataset=\"train\", output_name=\"imagenet_4class\", \n",
    "#                                                weights='imagenet')\n",
    "# x_val_img, y_val_img, _ = get_vgg_features(dataset=\"val\", output_name=\"imagenet_4class\", \n",
    "#                                            weights='imagenet')\n",
    "# augment_and_save(dataset=\"train\")\n",
    "# augment_and_save(dataset=\"val\")\n",
    "# x_train_aug, y_train_aug, _ = get_vgg_features(dataset=\"train\", use_augmentation=True, \n",
    "#                                        output_name=\"aug_places_4class\")\n",
    "# x_val_aug, y_val_aug, _ = get_vgg_features(dataset=\"val\", use_augmentation=True,\n",
    "#                                    output_name=\"aug_places_4class\")\n",
    "# x_train_img_aug, y_train_img_aug, _ = get_vgg_features(dataset=\"train\", use_augmentation=True, \n",
    "#                                                output_name=\"aug_imagenet_4class\", \n",
    "#                                                weights='imagenet')\n",
    "# x_val_img_aug, y_val_img_aug, _ = get_vgg_features(dataset=\"val\", use_augmentation=True,\n",
    "#                                            output_name=\"aug_imagenet_4class\", \n",
    "#                                            weights='imagenet')\n",
    "# x_test, y_test, _ = get_vgg_features(dataset=\"test\", output_name=\"places_4class\")\n",
    "# x_test_img, y_test_img, _ = get_vgg_features(dataset=\"test\", output_name=\"imagenet_4class\", weights='imagenet')\n",
    "# x_test365, y_test365, _ = get_vgg_features(dataset=\"test\", output_name=\"places_365\", weights='places_2')\n",
    "\n",
    "\n",
    "\n",
    "# Read pre-computed feature vectors from file system\n",
    "x_train, y_train = read_vgg_features(dataset=\"train\", input_name=\"places_4class\")\n",
    "x_val, y_val = read_vgg_features(dataset=\"val\", input_name=\"places_4class\")\n",
    "x_test, y_test = read_vgg_features(dataset=\"test\", input_name=\"places_4class\")\n",
    "\n",
    "x_train_img, y_train_img = read_vgg_features(dataset=\"train\", input_name=\"imagenet_4class\")\n",
    "x_val_img, y_val_img = read_vgg_features(dataset=\"val\", input_name=\"imagenet_4class\")\n",
    "x_test_img, y_test_img = read_vgg_features(dataset=\"test\", input_name=\"imagenet_4class\")\n",
    "\n",
    "x_train365, y_train365 = read_vgg_features(dataset=\"train\", input_name=\"places_365\")\n",
    "x_val365, y_val365 = read_vgg_features(dataset=\"val\", input_name=\"places_365\")\n",
    "x_test365, y_test365 = read_vgg_features(dataset=\"test\", input_name=\"places_365\")\n",
    "\n",
    "\n",
    "x_train_aug, y_train_aug = read_vgg_features(dataset=\"train\", input_name=\"aug_places_4class\")\n",
    "x_val_aug, y_val_aug = read_vgg_features(dataset=\"val\", input_name=\"aug_places_4class\")\n",
    "x_train_img_aug, y_train_img_aug = read_vgg_features(dataset=\"train\", input_name=\"aug_imagenet_4class\")\n",
    "x_val_img_aug, y_val_img_aug = read_vgg_features(dataset=\"val\", input_name=\"aug_imagenet_4class\")\n",
    "x_train_aug365, y_train_aug365 = read_vgg_features(dataset=\"train\", input_name=\"places_365_aug\")\n",
    "x_val_aug365, y_val_aug365 = read_vgg_features(dataset=\"val\", input_name=\"places_365_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR\n",
      "Best results: n_acc =     0.71, f1:     0.71\n",
      "Best results: n_acc =     0.73, f1:     0.73\n",
      "Best results: n_acc =     0.71, f1:     0.70\n",
      "SVMs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results: n_acc =     0.73, f1:     0.73\n",
      "Best results: n_acc =     0.75, f1:     0.75\n",
      "Best results: n_acc =     0.71, f1:     0.71\n",
      "Random Forests\n",
      "Best results: n_acc =   0.7313, f1:   0.7269\n",
      "Best results: n_acc =   0.7563, f1:   0.7514\n",
      "Best results: n_acc =   0.7312, f1:   0.7250\n",
      "Bagging and boosting\n",
      "Best results: n_acc =     0.69, f1:     0.69\n",
      "Best results: n_acc =     0.62, f1:     0.62\n",
      "Best results: n_acc =     0.74, f1:     0.74\n",
      "Best results: n_acc =     0.53, f1:     0.53\n",
      "Best results: n_acc =     0.71, f1:     0.71\n",
      "Best results: n_acc =     0.56, f1:     0.57\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This block tests several different classifiers, performing grid-search on the necessary parameters.\n",
    "# Might take a long time to run.\n",
    "#\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "def test_svms(x_train, y_train, x_val, y_val, verbose=False):\n",
    "    result_str = \"k = {:>8}, c = {:>8}, g = {:>8}, n_acc = {:>8.2f}, f1: {:>8.2f}\"\n",
    "    kernels = ['linear', 'rbf']\n",
    "    cs = [10**i for i in range(-1, 8)]\n",
    "\n",
    "    best_nacc = -1\n",
    "    best_f1 = -1\n",
    "\n",
    "    n_accs = []\n",
    "    f1s = []\n",
    "    for k in kernels:\n",
    "        for c in cs:\n",
    "            \n",
    "            if k == 'rbf':\n",
    "                gammas = [10**i for i in range(-8, 2)]\n",
    "            else:\n",
    "                gammas = [\"auto\"]\n",
    "            for g in gammas:\n",
    "                clf = SVC(C=c, kernel=k, gamma=g)\n",
    "                clf.fit(x_train, y_train)\n",
    "                preds = clf.predict(x_val)\n",
    "\n",
    "                n_acc = normalized_accuracy(y_val, preds)\n",
    "                f1 = f1_score(y_val, preds, average = 'macro')\n",
    "\n",
    "                best_f1 = np.max((best_f1, f1))\n",
    "                best_nacc = np.max((best_nacc, n_acc))\n",
    "                n_accs.append(n_acc)\n",
    "                f1s.append(f1)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(result_str.format(k, c,  g, n_acc, f1))\n",
    "\n",
    "    print(\"Best results: n_acc = {:>8.2f}, f1: {:>8.2f}\".format(best_nacc, best_f1))\n",
    "\n",
    "\n",
    "def test_random_forest(x_train, y_train, x_val, y_val, verbose=False):\n",
    "    result_str = \"crit = {:>8}, n = {:>8},  n_acc = {:>8.2f}, f1: {:>8.2f}\"\n",
    "    \n",
    "    criterion = ['gini', 'entropy']\n",
    "    num_trees = [10*i for i in range(1, 101)]\n",
    "    \n",
    "    best_nacc = -1\n",
    "    best_f1 = -1\n",
    "\n",
    "    n_accs = []\n",
    "    f1s = []\n",
    "    for c in criterion:\n",
    "        for n in num_trees:\n",
    "            \n",
    "            clf = RandomForestClassifier(n_estimators=n, criterion=c, n_jobs=-1, random_state=1107)\n",
    "            clf.fit(x_train, y_train)\n",
    "            preds = clf.predict(x_val)\n",
    "\n",
    "            n_acc = normalized_accuracy(y_val, preds)\n",
    "            f1 = f1_score(y_val, preds, average = 'macro')\n",
    "\n",
    "            best_f1 = np.max((best_f1, f1))\n",
    "            best_nacc = np.max((best_nacc, n_acc))\n",
    "            n_accs.append(n_acc)\n",
    "            f1s.append(f1)\n",
    "            if verbose:\n",
    "                print(result_str.format(c, n, n_acc, f1))\n",
    "\n",
    "    print(\"Best results: n_acc = {:>8.4f}, f1: {:>8.4f}\".format(best_nacc, best_f1))\n",
    "\n",
    "def test_log_r(x_train, y_train, x_val, y_val, multi_class='ovr', solver='liblinear', verbose=False):\n",
    "    result_str = \"LOGR: C = {:>8}, n_acc = {:>8.2f}, f1: {:>8.2f}\"\n",
    "\n",
    "    cs = [10**i for i in range(-10, 10)]\n",
    "    best_nacc = -1\n",
    "    best_f1 = -1\n",
    "    \n",
    "    n_accs = []\n",
    "    f1s = []\n",
    "    for c in cs:\n",
    "        log_r = LogisticRegression(C=c, multi_class=multi_class, solver=solver)\n",
    "        log_r.fit(x_train, y_train)\n",
    "        preds = log_r.predict(x_val)\n",
    "\n",
    "        n_acc = normalized_accuracy(y_val, preds)\n",
    "        f1 = f1_score(y_val, preds, average = 'macro')\n",
    "\n",
    "        best_f1 = np.max((best_f1, f1))\n",
    "        best_nacc = np.max((best_nacc, n_acc))\n",
    "        n_accs.append(n_acc)\n",
    "        f1s.append(f1)\n",
    "        if verbose:\n",
    "            print(result_str.format(c, n_acc, f1))\n",
    "       \n",
    "    print(\"Best results: n_acc = {:>8.2f}, f1: {:>8.2f}\".format(best_nacc, best_f1))\n",
    "\n",
    "\n",
    "def test_bagging(x_train, y_train, x_val, y_val, verbose=False):\n",
    "    result_str = \"n_est = {:>8},  n_acc = {:>8.4f}, f1: {:>8.4f}\"\n",
    "    num_est = [10 * i for i in range(1, 21)]\n",
    "\n",
    "    best_nacc = -1\n",
    "    best_f1 = -1\n",
    "\n",
    "    n_accs = []\n",
    "    f1s = [] \n",
    "\n",
    "    for n_est in num_est:\n",
    "        clf = BaggingClassifier(n_estimators=n_est)\n",
    "        clf.fit(x_train, y_train)\n",
    "        preds = clf.predict(x_val)\n",
    "\n",
    "        n_acc = normalized_accuracy(y_val, preds)\n",
    "        f1 = f1_score(y_val, preds, average = 'macro')\n",
    "\n",
    "        best_f1 = np.max((best_f1, f1))\n",
    "        best_nacc = np.max((best_nacc, n_acc))\n",
    "        n_accs.append(n_acc)\n",
    "        f1s.append(f1)\n",
    "        if verbose:\n",
    "            print(result_str.format(n_est, n_acc, f1))\n",
    "\n",
    "    print(\"Best results: n_acc = {:>8.2f}, f1: {:>8.2f}\".format(best_nacc, best_f1))\n",
    "\n",
    "def test_adaboost(x_train, y_train, x_val, y_val, verbose=False):\n",
    "    result_str = \"n_est = {:>8},  n_acc = {:>8.4f}, f1: {:>8.4f}\"\n",
    "    num_est = [50 * i for i in range(1, 21)]\n",
    "\n",
    "    best_nacc = -1\n",
    "    best_f1 = -1\n",
    "\n",
    "    n_accs = []\n",
    "    f1s = [] \n",
    "\n",
    "    for n_est in num_est:\n",
    "        clf = AdaBoostClassifier(n_estimators=n_est)\n",
    "        clf.fit(x_train, y_train)\n",
    "        preds = clf.predict(x_val)\n",
    "\n",
    "        n_acc = normalized_accuracy(y_val, preds)\n",
    "        f1 = f1_score(y_val, preds, average = 'macro')\n",
    "\n",
    "        best_f1 = np.max((best_f1, f1))\n",
    "        best_nacc = np.max((best_nacc, n_acc))\n",
    "        n_accs.append(n_acc)\n",
    "        f1s.append(f1)\n",
    "        if verbose:\n",
    "            print(result_str.format(n_est, n_acc, f1))\n",
    "\n",
    "    print(\"Best results: n_acc = {:>8.2f}, f1: {:>8.2f}\".format(best_nacc, best_f1))\n",
    "\n",
    "    \n",
    "#\n",
    "# Test each classifier vs each dataset in the validation set.\n",
    "# Augmented sets are commented as they take a long time to run\n",
    "#\n",
    "print(\"LogR\")\n",
    "test_log_r(x_train, y_train, x_val, y_val)\n",
    "test_log_r(x_train_img, y_train_img, x_val_img, y_val_img)\n",
    "test_log_r(x_train365, y_train365, x_val365, y_val365)\n",
    "# test_log_r(x_train_aug, y_train_aug, x_val, y_val)\n",
    "# test_log_r(x_train_img_aug, y_train_img_aug, x_val_img, y_val_img)\n",
    "# test_log_r(x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365)\n",
    "\n",
    "print(\"SVMs\")\n",
    "test_svms(x_train, y_train, x_val, y_val)\n",
    "test_svms(x_train_img, y_train_img, x_val_img, y_val_img)\n",
    "test_svms(x_train365, y_train365, x_val365, y_val365)\n",
    "# test_svms(x_train_aug, y_train_aug, x_val, y_val)\n",
    "# test_svms(x_train_img_aug, y_train_img_aug, x_val_img, y_val_img)\n",
    "# test_svms(x_train_aug365, y_train_aug365, x_val365, y_val365)\n",
    "\n",
    "print(\"Random Forests\")\n",
    "test_random_forest(x_train, y_train, x_val, y_val)\n",
    "test_random_forest(x_train_img, y_train_img, x_val_img, y_val_img)\n",
    "test_random_forest(x_train365, y_train365, x_val365, y_val365)\n",
    "# test_random_forest(x_train_aug, y_train_aug, x_val, y_val)\n",
    "# test_random_forest(x_train_img_aug, y_train_img_aug, x_val_img, y_val_img)\n",
    "# test_random_forest(x_train_aug365, y_train_aug365, x_val365, y_val365)\n",
    "\n",
    "print(\"Bagging and boosting\")\n",
    "test_bagging(x_train, y_train, x_val, y_val)\n",
    "test_adaboost(x_train, y_train, x_val, y_val)\n",
    "test_bagging(x_train_img, y_train_img, x_val_img, y_val_img)\n",
    "test_adaboost(x_train_img, y_train_img, x_val_img, y_val_img)\n",
    "test_bagging(x_train365, y_train365, x_val365, y_val365)\n",
    "test_adaboost(x_train365, y_train365, x_val365, y_val365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Given the best classifiers obtained above in the validation set, this block provides utilities\n",
    "# to train, store and read them from disk\n",
    "#\n",
    "\n",
    "# Creates an array of dictionaries containing the optimal parameteres of each classifier\n",
    "def get_classif_params():\n",
    "    # [Type of classif, C, kernel type, gamma]\n",
    "    params = []\n",
    "\n",
    "    # no aug - places hybrid\n",
    "    params.append({\"classif\": \"svm\", \"k\": 'rbf', \"c\": 1, \"g\": 0.0001, \"aug\": False, \"net\": \"places\"})\n",
    "    params.append({\"classif\": \"logr\", \"c\": 0.001, \"aug\": False, \"net\": \"places\"})\n",
    "    params.append({\"classif\": \"rf\", \"crit\": 'gini', \"n\": 170, \"aug\": False, \"net\": \"places\"})\n",
    "\n",
    "    # no aug - imagenet\n",
    "    params.append({\"classif\": \"svm\", \"k\": 'rbf', \"c\": 1, \"g\": 0.0001, \"aug\": False, \"net\": \"imagenet\"})\n",
    "    params.append({\"classif\": \"logr\", \"c\": 10, \"aug\": False, \"net\": \"imagenet\"})\n",
    "    params.append({\"classif\": \"rf\", \"crit\": 'gini', \"n\": 790, \"aug\": False, \"net\": \"imagenet\"})\n",
    "\n",
    "    # no aug - places365\n",
    "    params.append({\"classif\": \"svm\", \"k\": 'rbf', \"c\": 1, \"g\": 0.001, \"aug\": False, \"net\": \"365\"})\n",
    "    params.append({\"classif\": \"logr\", \"c\": 0.01, \"aug\": False, \"net\": \"365\"})\n",
    "    params.append({\"classif\": \"rf\", \"crit\": 'entropy', \"n\": 560, \"aug\": False, \"net\": \"365\"})\n",
    "    \n",
    "    # aug - places\n",
    "    params.append({\"classif\": \"svm\", \"k\": 'rbf', \"c\": 0.1, \"g\": 0.0001, \"aug\": True, \"net\": \"places\"})\n",
    "    params.append({\"classif\": \"logr\", \"c\": 0.0001, \"aug\": True, \"net\": \"places\"})\n",
    "    params.append({\"classif\": \"rf\", \"crit\": 'gini', \"n\": 220, \"aug\": True, \"net\": \"places\"})\n",
    "    \n",
    "    # aug - imagenet\n",
    "    params.append({\"classif\": \"svm\", \"k\": 'rbf', \"c\": 0.1, \"g\": 0.0001, \"aug\": True, \"net\": \"imagenet\"})\n",
    "    params.append({\"classif\": \"logr\", \"c\": 0.0001, \"aug\": True, \"net\": \"imagenet\"})\n",
    "    params.append({\"classif\": \"rf\", \"crit\": 'gini', \"n\": 600, \"aug\": True, \"net\": \"imagenet\"})\n",
    "    \n",
    "    # aug - places365\n",
    "    params.append({\"classif\": \"svm\", \"k\": 'rbf', \"c\": 10000, \"g\": 1e-7, \"aug\": True, \"net\": \"365\"})\n",
    "    params.append({\"classif\": \"logr\", \"c\": 0.001, \"aug\": True, \"net\": \"365\"})\n",
    "    params.append({\"classif\": \"rf\", \"crit\": 'entropy', \"n\": 600, \"aug\": True, \"net\": \"365\"})\n",
    "    return params\n",
    "\n",
    "# Train classifiers and save them to disk\n",
    "def train_and_save_best_classif(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img, \n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365, preffix=\"\", verbose=False):\n",
    "\n",
    "    params = get_classif_params()\n",
    "    for p in params:\n",
    "        xt, yt, xv, yv = get_sets(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img, \n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug, \n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365, p)\n",
    "\n",
    "        if p[\"classif\"] == 'svm':\n",
    "            classif = SVC(C=p[\"c\"], kernel=p[\"k\"], gamma=p[\"g\"])\n",
    "            classif.fit(xt, yt)\n",
    "        elif p[\"classif\"]  == 'logr':\n",
    "            classif = LogisticRegression(C=p[\"c\"], multi_class='ovr', solver='liblinear')\n",
    "            classif.fit(xt, yt)\n",
    "        elif p[\"classif\"] == \"rf\":\n",
    "            classif = RandomForestClassifier(n_estimators=p[\"n\"], criterion=p[\"crit\"], n_jobs=-1, random_state=1107)\n",
    "            classif.fit(xt, yt)\n",
    "       \n",
    "        pickle.dump(classif, open('{}{}_{}.pickle'.format(PICKLE_PATH, preffix, p), 'wb'))\n",
    "        \n",
    "        if verbose:\n",
    "            preds = classif.predict(xv)\n",
    "            n_acc = normalized_accuracy(yv, preds)\n",
    "            f1 = f1_score(y_val, preds, average = 'macro')\n",
    "            print(\"params: {}, n_acc: {:.4f}, f1 macro: {:.4f}\".format(p, n_acc, f1))\n",
    "\n",
    "# Load pre-trained classifiers from disk\n",
    "def load_best_classif(preffix):\n",
    "    params = get_classif_params()\n",
    "    classifs = []\n",
    "    for p in params:\n",
    "        classifs.append(pickle.load(open('{}{}_{}.pickle'.format(PICKLE_PATH, preffix, p), 'rb')))\n",
    "    return classifs\n",
    "\n",
    "# Auxiliary method that returns the training and validation sets for the corresponding classifier\n",
    "def get_sets(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img, \n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365, p):\n",
    "    aug, net = p[\"aug\"], p[\"net\"]\n",
    "    \n",
    "    if aug == False and net == \"places\":\n",
    "        xt, yt, xv, yv = x_train, y_train, x_val, y_val\n",
    "    elif aug == False and net == \"imagenet\":\n",
    "        xt, yt, xv, yv = x_train_img, y_train_img, x_val_img, y_val_img\n",
    "    elif aug == False and net == \"365\":\n",
    "        xt, yt, xv, yv = x_train365, y_train365, x_val365, y_val365\n",
    "    elif aug == True and net == \"places\":\n",
    "        xt, yt, xv, yv = x_train_aug, y_train_aug, x_val, y_val\n",
    "    elif aug == True and net == \"imagenet\":\n",
    "        xt, yt, xv, yv = x_train_img_aug, y_train_img_aug, x_val_img, y_val_img\n",
    "    elif aug == True and net == \"365\":\n",
    "        xt, yt, xv, yv = x_train_aug365, y_train_aug365, x_val365, y_val365\n",
    "        \n",
    "    return xt, yt, xv, yv\n",
    " \n",
    "# Auxiliary method that returns the testing for the corresponding classifier\n",
    "def get_test_sets(x_test, y_test, x_test_img, y_test_img, x_test365, y_test365, params):\n",
    "    if params[\"net\"] == \"places\":\n",
    "        xt, yt = x_test, y_test\n",
    "    elif params[\"net\"] == \"imagenet\":\n",
    "        xt, yt = x_test_img, y_test_img\n",
    "    elif params[\"net\"] == \"365\":\n",
    "        xt, yt = x_test365, y_test365\n",
    "    return xt, yt\n",
    "    \n",
    "# Save our classifiers to disk, for reusal later\n",
    "train_and_save_best_classif(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img,\n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365, preffix=\"4class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: n_acc: 0.73, f1 macro: 0.73\n",
      "LogisticRegression: n_acc: 0.70, f1 macro: 0.70\n",
      "RandomForestClassifier: n_acc: 0.73, f1 macro: 0.73\n",
      "SVC: n_acc: 0.73, f1 macro: 0.73\n",
      "LogisticRegression: n_acc: 0.72, f1 macro: 0.72\n",
      "RandomForestClassifier: n_acc: 0.72, f1 macro: 0.72\n",
      "SVC: n_acc: 0.64, f1 macro: 0.63\n",
      "LogisticRegression: n_acc: 0.61, f1 macro: 0.60\n",
      "RandomForestClassifier: n_acc: 0.62, f1 macro: 0.61\n",
      "SVC: n_acc: 0.72, f1 macro: 0.72\n",
      "LogisticRegression: n_acc: 0.71, f1 macro: 0.71\n",
      "RandomForestClassifier: n_acc: 0.72, f1 macro: 0.72\n",
      "SVC: n_acc: 0.71, f1 macro: 0.71\n",
      "LogisticRegression: n_acc: 0.72, f1 macro: 0.72\n",
      "RandomForestClassifier: n_acc: 0.73, f1 macro: 0.73\n",
      "SVC: n_acc: 0.59, f1 macro: 0.58\n",
      "LogisticRegression: n_acc: 0.61, f1 macro: 0.61\n",
      "RandomForestClassifier: n_acc: 0.63, f1 macro: 0.61\n"
     ]
    }
   ],
   "source": [
    "# Now, check the accuracy of our classifiers in the test dataset\n",
    "def check_test_accuracy(x_test, y_test, x_test_img, y_test_img, x_test365, y_test365):\n",
    "    classifs = load_best_classif(\"4class\")\n",
    "    params = get_classif_params()\n",
    "    confs = []\n",
    "    for i, c in enumerate(classifs):\n",
    "        xt, yt = get_test_sets(x_test, y_test, x_test_img, y_test_img, x_test365, y_test365, params[i])\n",
    "            \n",
    "        preds = c.predict(xt)\n",
    "        n_acc = normalized_accuracy(yt, preds)\n",
    "        f1 = f1_score(yt, preds, average = 'macro')                     \n",
    "        print(\"{}: n_acc: {:.2f}, f1 macro: {:.2f}\".format(c.__class__.__name__, n_acc, f1))\n",
    "    return preds, confs\n",
    "\n",
    "check_test_accuracy(x_test, y_test, x_test_img, y_test_img, x_test365, y_test365);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Fusion code - Majority vote and meta-learning\n",
    "#\n",
    "\n",
    "# Returns predictions for all classifiers and all datasets. Used for majority voting and meta-learning\n",
    "def get_all_preds(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img,\n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365,\n",
    "                                x_test=None, y_test = None, x_test_img = None, y_test_img = None,\n",
    "                                x_test365=None, y_test365 = None, tests=False):\n",
    "\n",
    "    classifs = load_best_classif(\"4class\")\n",
    "    params = get_classif_params()\n",
    "    \n",
    "    if tests:\n",
    "        preds = np.zeros((len(classifs), x_test.shape[0]))\n",
    "    else:\n",
    "        preds = np.zeros((len(classifs), x_val.shape[0]))\n",
    "    for i, c in enumerate(classifs):\n",
    "        xt, yt, xv, yv = get_sets(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img,\n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365, params[i])\n",
    "        \n",
    "        if tests:\n",
    "            xv, yv = get_test_sets(x_test, y_test, x_test_img, y_test_img, x_test365, y_test365, params[i])\n",
    "        preds[i] = c.predict(xv)\n",
    "    return preds\n",
    "\n",
    "# Same for the train data-set\n",
    "def get_preds_train(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img,\n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365):\n",
    "    classifs = load_best_classif(\"4class\")\n",
    "    params = get_classif_params()\n",
    "    preds_train = np.zeros((int(len(classifs)/2), x_train.shape[0]))\n",
    "\n",
    "    for i, c in enumerate(classifs):\n",
    "        if params[i][\"aug\"] == False:\n",
    "            xt, yt, xv, yv = get_sets(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img,\n",
    "                                    x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                    x_train365, y_train365, x_val365, y_val365, \n",
    "                                    x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365, params[i])\n",
    "            preds_train[i] = c.predict(xt)\n",
    "    return preds_train\n",
    "\n",
    "# Test majority voting\n",
    "def test_majority_vote(y_val, preds):\n",
    "    pred_int = preds.astype('uint32')\n",
    "    majority_preds = np.zeros(preds.shape[1])\n",
    "    for i in range(preds.shape[1]):\n",
    "        majority_preds[i] = np.argmax(np.bincount(pred_int[:, i])) \n",
    "    \n",
    "    n_acc = normalized_accuracy(y_val, majority_preds)\n",
    "    f1 = f1_score(y_val, majority_preds, average = 'macro')\n",
    "    print(\"Majority vote: n_acc: {:.2f}, f1: {:.2f}\".format(n_acc, f1))\n",
    "\n",
    "\n",
    "# Test meta-learning\n",
    "def test_meta_learning(y_train, y_val, preds_train, preds_val, verbose=False):\n",
    "    classifs = load_best_classif(\"4class\")\n",
    "\n",
    "    x_meta_train = np.transpose(preds_train)\n",
    "    x_meta_val = np.transpose(preds_val)\n",
    "    cs = [10**i for i in range(-5, 5)]\n",
    "    gammas = [10**i for i in range(-8, 8)]\n",
    "    \n",
    "    best_f1, best_nacc = -1, -1\n",
    "\n",
    "    for c in cs:\n",
    "        for g in gammas:\n",
    "            clf = SVC(C=c, kernel='rbf', gamma=g)\n",
    "            clf.fit(x_meta_train, y_train)\n",
    "            preds = clf.predict(x_meta_val)\n",
    "            n_acc = normalized_accuracy(y_val, preds)\n",
    "            f1 = f1_score(y_val, preds, average = 'macro')\n",
    "            best_f1 = np.max((best_f1, f1))\n",
    "            best_nacc = np.max((best_nacc, n_acc))\n",
    "            if verbose:\n",
    "                print(\"c: {}, n_acc: {:.2f}, f1: {:.2f}\".format(c, n_acc, f1))\n",
    "    print(\"Meta-Learning: n_acc: {:.2f}, f1: {:.2f}\".format(best_nacc, best_f1))\n",
    "            \n",
    "# Get predictions for validation\n",
    "preds_train = get_preds_train(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img,\n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365)\n",
    "\n",
    "# Get predictions for validation\n",
    "preds_val = get_all_preds(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img,\n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365)\n",
    "\n",
    "# And for tests\n",
    "preds_test = get_all_preds(x_train, y_train, x_val, y_val, x_train_img, y_train_img, x_val_img, y_val_img,\n",
    "                                x_train_aug, y_train_aug, x_train_img_aug, y_train_img_aug,\n",
    "                                x_train365, y_train365, x_val365, y_val365, \n",
    "                                x_train_aug365, y_train_aug365, x_val_aug365, y_val_aug365,\n",
    "                                x_test, y_test, x_test_img, y_test_img, x_test365, y_test365, tests=True)\n",
    "\n",
    "# Test majority voting and meta learning on validation set - First for non-augmented classifiers only,\n",
    "# then for augmented only, and then for both\n",
    "print(\"Validation set\")\n",
    "test_majority_vote(y_val, preds_val[:9,:])\n",
    "test_majority_vote(y_val, preds_val[9:,:])\n",
    "test_majority_vote(y_val, preds_val)\n",
    "test_meta_learning(y_train, y_val, preds_train, preds_val[:9])\n",
    "\n",
    "\n",
    "# Same for test dataset\n",
    "print(\"\\nTest Dataset\")\n",
    "test_majority_vote(y_test, preds_test[:9,:])\n",
    "test_majority_vote(y_test, preds_test[9:,:])\n",
    "test_majority_vote(y_test, preds_test)\n",
    "test_meta_learning(y_train, y_test, preds_train, preds_test[:9])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
